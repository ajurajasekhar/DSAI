{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLR model for predicting the auction price of IPL players"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.set_printoptions(precision=2, linewidth=100)\n",
    "#Number of digits of precision for floating point output (default 8).\n",
    "#The number of characters per line for the purpose of inserting line breaks (default 75)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "#statsmodels provides classes and functions for the estimation \n",
    "#of many different statistical models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sklearn features various classification, regression and clustering algorithms\n",
    "from sklearn.model_selection import train_test_split\n",
    "#for splitting data arrays into two subsets: for training data and for testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "%matplotlib inline\n",
    "#%matplotlib inline makes your plot outputs appear and be stored within the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/Foridur3210/IPL-Dataset-Player-price-prediction/blob/master/IPL%20IMB381IPL2013.csv\n",
    "ipl_df = pd.read_csv( 'IPL IMB381IPL2013.csv' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipl_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipl_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ipl_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipl_df.columns"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Note that some columns like Sl.no are not features. So we select the features which are likely to predict the auction price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_features = ipl_df.columns\n",
    "X_features = ['AGE', 'COUNTRY', 'PLAYING ROLE',\n",
    "'T-RUNS', 'T-WKTS', 'ODI-RUNS-S', 'ODI-SR-B',\n",
    "'ODI-WKTS', 'ODI-SR-BL', 'CAPTAINCY EXP', 'RUNS-S',\n",
    "'HS', 'AVE', 'SR-B', 'SIXERS', 'RUNS-C', 'WKTS',\n",
    "'AVE-BL', 'ECON', 'SR-BL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_features"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Some features like age, country, playing role, captaincy exp, etc. are categorical. Categorical variables need to be coded into dummy variables for model building."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding using dummy variables"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "We will find the unique values for all categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipl_df['AGE'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipl_df['COUNTRY'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipl_df['PLAYING ROLE'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipl_df['CAPTAINCY EXP'].unique()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The 'PLAYING ROLE' categorical variable has 4 categories such as allrounder, batsman, bowler,and  wicket-keeper, and so it needs 3 dummy variables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pd.get_dummies(ipl_df['PLAYING ROLE'])[0:10]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Each of the dummy variable is used to represent whether an observation belongs to a category or not. We create only n-1 dummy variables. The absense of all n-1 dummy variables represent the first dummy variable (set by drop-first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_features = ['AGE', 'COUNTRY', 'PLAYING ROLE', 'CAPTAINCY EXP']\n",
    "ipl_encoded_df = pd.get_dummies( ipl_df[X_features],\n",
    "columns = category_features, drop_first = True)\n",
    "ipl_encoded_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipl_encoded_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dividing data into training and test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train_test_split(*arrays, test_size=None, train_size=None, random_state=None, shuffle=True, stratify=None). The test_size (or train_size) should be between 0.0 and 1.0 and represent the proportion of the dataset to include in the test(or train) split. Need to specify only one of them.\n",
    "If train_size is 0.8, the test-size will be set to 0.2. If both are  None, the test_size (train_size) will be set to 0.25 (0.75) by default.\n",
    "random-state passes an int for reproducible output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_features = ipl_encoded_df.columns\n",
    "X = sm.add_constant( ipl_encoded_df )\n",
    "#dependent variable\n",
    "Y = ipl_df['SOLD PRICE']\n",
    "train_X, test_X, train_y, test_y = train_test_split( X,Y,train_size = 0.8,random_state = 41)\n",
    "#default is 0.75\n",
    "# 52 is a seed  for reproducibility of randomness "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "fit the model using OLS method and pass train_y and train_X as parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ipl_model1 = sm.OLS(train_y, train_X).fit()\n",
    "ipl_model1.summary2()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "With p-value <0.05, only few features have come out as significant. This is a bit counter intuitive.\n",
    "We suspect the presence of multicollinearity and take corrective actions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Multicollinearity"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "We will use the variance inflation Factor (VIF)  for identifying the existence of multi-collinearity.\n",
    "Check if  VIF value greater than 4. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "def compute_vif_factors( X ):\n",
    "   # X_matrix = X.as_matrix()\n",
    "    vif_factors = pd.DataFrame()\n",
    "    vif_factors['column'] = X.columns\n",
    "    vif = [ variance_inflation_factor( X.values, i ) for i in range( X.shape[1] ) ]\n",
    "    vif_factors['vif'] = vif\n",
    "    return vif_factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vif_factors = compute_vif_factors( X[X_features] )\n",
    "vif_factors"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "There are many features with VLF value  more than 4. We generate a correlation heatmap for them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "large_vif_features = vif_factors[vif_factors.vif > 4].column\n",
    "plt.figure( figsize = (12,10) )\n",
    "sn.heatmap( X[large_vif_features].corr(), annot = True );\n",
    "plt.title( \"Heatmap to check multicollinearity\");"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Observe that the correlation between 'T-RUNS' and 'ODI-RUNS-S' is 0.89 which means that they are highly correlated We need to retain only one of them. So remove one feature from all such correlated pairs.  This needs to be done iteratively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_removed = ['T-RUNS', 'T-WKTS', 'RUNS-S', 'HS','AVE', 'RUNS-C', 'SR-B', 'AVE-BL',\n",
    "'ECON', 'ODI-SR-B', 'ODI-RUNS-S', 'AGE_2', 'SR-BL']\n",
    "X_revised_features = list( set(X_features) - set(columns_removed) )"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The features without multicollinearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_vif_factors( X[X_revised_features] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All VIF values are less than 4.\n",
    "Now build a new model with these features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_X = train_X[X_revised_features]\n",
    "ipl_model2 = sm.OLS(train_y, train_X).fit()\n",
    "ipl_model2.summary2()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Now, based on the p-values (<0.05), COUNTRY_IND, COUNTRY_ENG, SIXERS, ODI-WKTS have come out statiscally significant.\n",
    "Now we rebuild the model with these features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "significant_features = ['COUNTRY_IND', 'COUNTRY_ENG', 'SIXERS', 'ODI-WKTS']\n",
    "train_X = train_X[significant_features]\n",
    "ipl_model3 = sm.OLS(train_y, train_X).fit()\n",
    "ipl_model3.summary2()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "All the features are statistically significant, as the p-value is less than 0.05\n",
    "The overall model is significant  as the  F-statistics probability is also less than 0.05. \n",
    "The model can explain 74.3% of the variance in SOLD PRICE as the R-squared value is 0.743.\n",
    "The adjusted R-squared value is 0.732. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test for Normality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ppplot = sm.ProbPlot(ipl_model3.resid, fit=True);\n",
    "plt.figure( figsize = (8, 6) );\n",
    "ppplot.ppplot( line='45' );\n",
    "plt.title(\"Normal p-p plot\");\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The residuals appear to be almost normally distributed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test for Homoscedasticity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.scatter( ipl_model3.fittedvalues,ipl_model3.resid,marker=\"o\")\n",
    "plt.xlabel(\"Standardized predicted values\")\n",
    "plt.ylabel(\"Standardized residual values\");"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "No funnel pattern observed (no signs of heteroscedasticity). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test for Autocorrelation between error terms"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "A Durbin-Watson statistic close to 2 indicate the absense of autocorrelation.\n",
    "The Durbin-Watson statisic is 1.832 for model3. So we can say that there is no autocorrelation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making the Predictions and Measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_y = np.power( ipl_model3.predict( test_X[train_X.columns] ), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "import numpy\n",
    "np.sqrt(mean_squared_error(pred_y,test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "np.abs(r2_score(pred_y, test_y))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The R-squared for the training data is 0.743 and for the test data is 0.87. So no overfitting indication. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detecting Influential Observations"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "A leverage value of more than 3(k+1)/n where k is the number of features in the model and n is the sample size, is considered as a highly influential observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = train_X.shape[1]\n",
    "n = train_X.shape[0]\n",
    "print( \"Number of variables:\", k, \" and number of observations:\", n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leverage_threshold = 3*((k + 1)/n)\n",
    "print( \"Threshold for leverage value: \", round(leverage_threshold, 3) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.graphics.regressionplots import influence_plot\n",
    "fig, ax = plt.subplots( figsize=(8,6) )\n",
    "influence_plot( ipl_model3, ax = ax )\n",
    "plt.title( \"Leverage Value Vs Residuals\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Observations with high leverage value (>0.2) can be removed from the training data set (i.e., 26, 58, and 83)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipl_df[ipl_df.index.isin( [26, 58, 83] )]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X_revised = train_X.drop( [26, 58, 83], axis = 0)\n",
    "train_y_revised = train_y.drop( [26, 58, 83], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = train_X_revised\n",
    "train_y = train_y_revised\n",
    "ipl_model4 = sm.OLS(train_y, train_X).fit()\n",
    "ipl_model4.summary2()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Making  Predictions and Measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_y = np.power( ipl_model4.predict( test_X[train_X.columns] ), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "np.sqrt(mean_squared_error(test_y, pred_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "np.abs(r2_score(pred_y, test_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming the Response Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = np.sqrt( train_y )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ipl_model5 = sm.OLS(train_y, train_X).fit()\n",
    "ipl_model5.summary2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ppplot = sm.ProbPlot( ipl_model5.resid, fit=True );\n",
    "plt.figure( figsize = (8, 6) );\n",
    "ppplot.ppplot( line='45' );\n",
    "plt.title(\"Normal p-p plot of Regression Standardized Residuals\");\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_y = np.power( ipl_model5.predict( test_X[train_X.columns] ), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "np.sqrt(metrics.mean_squared_error(pred_y, test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "np.abs(r2_score(pred_y, test_y))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The R-squared for the training data is 0.796 and for the test data is 0.253. This is an overfitting indication. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
